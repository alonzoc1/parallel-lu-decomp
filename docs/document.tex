\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{physics}
\usepackage{MnSymbol}
\usepackage{float}

\usepackage{listings} %for listings of the source code

% Some definitions for using the listing package.
% When we reference 'codegreen', it will be the RGB color defined below.
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Also for the listings, this will make the code listing look like default MATLAB
%\lstdefinestyle{mystyle}{
%	backgroundcolor=\color{backcolour},
%	commentstyle=\color{codegreen},
%	keywordstyle=\color{magenta},
%	numberstyle=\tiny\color{codegray},
%	stringstyle=\color{codepurple},
%	basicstyle=\footnotesize,
%	breakatwhitespace=false,
%	breaklines=true,
%	captionpos=b,
%	keepspaces=true,
%	numbers=left,
%	numbersep=5pt,
%	showspaces=false,
%	showstringspaces=false,
%	showtabs=false,
%	tabsize=2
%}
\lstset{language=C++,
	basicstyle=\footnotesize,
	keywordstyle=\color{blue}\ttfamily,
	stringstyle=\color{red}\ttfamily,
	commentstyle=\color{green}\ttfamily,
	morecomment=[l][\color{magenta}]{\#},
    backgroundcolor=\color{backcolour},
    tabsize=2,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
    numbers=left,
    numbersep=5pt
}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{COMP-605}
\newcommand\hwnumber{Final Report}                  % <-- homework number
\newcommand\NetIDa{Raul L Bernal-Gonzalez}       % <-- NetID of person #1
\newcommand\NetIDb{Alonzo Castanon}           % <-- NetID of person #2 (Comment this line out for problem sets)

\pagestyle{fancyplain}
\headheight 15pt
\lhead{\NetIDa}
\lhead{\NetIDa\\\NetIDb}                 % <-- Comment this line out for problem sets (make sure you are person #1)
\chead{\textbf{\Large  \hwnumber}}
\rhead{\course \\ \today}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Parallelization of LU Decomposition} % Note the asteric so you do not get an additional number

LU-decomposition is a popular method of solving a linear system of equations with out having to invert a matrix. Any invertible matrix \(A\) can be factorized into an lower and upper triangular part. These \(L\) and \(U\) matrices can then be used to solve the \(A\vec{x} = \vec{b} \). This method is illustrated in the following:

\begin{align}
		&A\vec{x} = \vec{b} \\
	&(LU)\vec{x} = \vec{b} \\
	&L\vec{y} = \vec{b} \\
	&\vec{y} = L^{-1}\vec{b} \\
	&\vec{y} = U\vec{x}
\end{align}

Two popular algorithms for LU decomposition are Crout's and Gauss-Elimination.

\section{Crout's Algorithm}
   This method solves for the upper triangular part of the LU decomposition in a ``$\righthalfcup$" shape. It first computes the upper triangular part then then uses those values to create the lower triangular part. The algorithm iterates through columns of the matrix, going down each row of the column. The algoritms solves equations \ref{beta} and \ref{alpha}. This method is depicted in figure \ref{fig:crout}. The algorithm is ``in place'' with each iteration replacing the data in \(A\) with the LU decomposition. This is illustrated in equation \ref{lu-matrix}

\begin{align}
   \beta_{ij} = a_{ij} - \sum_{k=1}^{i-1} \alpha_{ik}\beta_{kj} \label{beta} \\
   \alpha_{ij} = {1 \over \beta_{jj}} \left(a_{ij} - \sum_{k=1}^{j-1} \alpha_{ik}\beta_{kj}\right) \label{alpha}
\end{align}

   \begin{align}\label{lu-matrix}
   	\mqty[\xmat*{a}{4}{4}] \rightarrow  \mqty[\beta_{11} & \beta_{12} & \beta_{13} & \beta_{14} \\
   	\alpha_{21} & \beta_{22} & \beta_{23} & \beta_{24} \\
   	\alpha_{31} & \alpha_{32} & \beta_{33} & \beta_{34} \\
   	\alpha_{41} & \alpha_{42} & \alpha_{43} & \beta_{44} \\]
   \end{align}

   \begin{figure}[H]
   	\centering
   	\includegraphics[width=0.7\linewidth]{"/Users/raul/Desktop/Screenshot 2024-05-02 at 7.55.50â€¯PM"}
    \caption{Numerical Recipes in C 2nd Edition}
   	\label{fig:crout}
   \end{figure}


	\subsection{Parallelization OpenMP}
	We decided to apply parallelization to the Numerical Recipes version of the Crouts's algorithm. The parallelization of this algorithm is fairly straight forward with OpenMP. We parallelized over the inner loops that go through the rows of each column. This was the best way to parallelized with OpenMP because it minimizes communication between threads. If we had parallelized over the main loop that iterates over the columns, we would have had to pass messages between threads because we need the all the columns \(< j\) to build the \(j^{th}\) column. The code for the openMP parallelization is shown below.

    \lstset{caption={Crout's algorithm parallelize with OpenMP. Adapted from Numerical Recipes}}
     \begin{lstlisting}
#pragma omp parallel for default(none) private(i, big) shared(scaling, a, n)
for (i = 0; i < n; i++) {
// Find the max abs(value) of each row
// detect if matrix is singular
    big = a.row(i).cwiseAbs().maxCoeff();
    if (big == 0.0) {
        printf("Singular matrix in routine ludcmp");
        exit(1);
    }
    scaling[i] = 1.0 / big;
}
// main crout's loop. iterate by column
for (j = 0; j < n; j++) {
    // build upper trianfular part up to j-1 row since eache thread will have its own set of i's,
    // no overlap the a(k,j) term would have already been build in previous iterations of j
#pragma omp parallel for default(none) private(i, k, sum) shared(a, j) schedule(dynamic)
  	for (i = 0; i < j; i++) {
  		sum = a(i, j);
  		for (k = 0; k < i; k++)
  			sum -= a(i, k) * a(k, j); // we are not reducing to sum
  		a(i, j) = sum;
  	}
  	// continue to build upper triangular part from i = j up to n rows
 #pragma omp parallel for default(none) private(i, sum, k) shared(a, j, n, big, imax, cout)
  	for (i = j; i < n; i++) {
  		sum = a(i, j);
  		for (k = 0; k < j; k++)
  			sum -= a(i, k) * a(k, j);
  		a(i, j) = sum;
  	}
  	// look for pivot. This area is not worth parallelizing since we will have to make the
  	// conditional a critical section. We interested in the row-index of max(big), not it's value.
  	// the operatio is scalar-scalar multiplication
  	big = 0.0;
  	for (i = j; i < n; i++) {
  		dum = scaling[i] * abs(a(i, j));
  		if (dum >= big) {
  			big = dum;
  			imax = i;
  		}
  	}
  	// swap pivot rows. No parallelization
  	if (j != imax) {
  		a.row(imax).swap(a.row(j));
  		scaling[imax] = scaling[j];
  	}
  	indx[j] = imax; // needed for backsubtitution if solveing Ax = b
  	if (a(j, j) == 0.0){
        printf("Singular matrix in routine ludcmp");
        exit(1);
      }
  	if (j != n - 1) {
  		dum = (1.0 / a(j, j));
  		// this last part build lower triangular part using the values we calculated above
#pragma omp parallel for default(none) private(i, k) shared(j, n, a, dum)
  		for (i = j + 1; i < n; i++)
  		a(i, j) *= dum;
  	}
  }
    \end{lstlisting}

The first loop simply finds the max absolute value of each row and stores it for future use. The first loop uses a dynamic scheduling because the number of iterations in the inner loops depend on the outer index; therefore threads with larger assigned indices will do less iterations. The pivoting part was left in serial because the conditional statement needs to be run by a single thread at a time to ensure the last thread to update imax is the thread with the max value. The swapping of the rows is also easier to handle in serial.

One problem I ran into was the if-statement checking for the index of the pivot. Originally, these statement was inside the \(i=j\) loop inside an omp critical region. This caused a bug in the code where the output would sometimes be wrong. After extracting the if-statement to its own unparalleled loop, the code worked again consistently. The problem could have been that imax was not always the index of the global max value, but instead the index of the max value of the last thread that updated it.

\subsection{Performance and Results}
The code was tested against Eigen's C++ Library LU decomposition and the serial version of the above code for correctness with random matrices. The times were also measured using random matrices of size \(64, 256, 512,\) and \(1024\). LU decomposition was performed 3 times on each matrix and the minimum time was recorded. The results can be seen in figures \ref{fig:speed-up-vs} and \ref{fig:efficiency-vs}. Overall, there was a noticeable speed-up in the parallel version. Since Tuckoo's CPU have 16 cores each, it makes sense that 16 is the optimal thread count. The efficiency follows an expected downward path. The smallest n value has the steepest curve while the largest n has the highest efficiency. Since Tuckoo's CPU do not have 32 physical threads, there is dramatic decrease of both speed-up and efficiency at 32 threads. Despite this, data before 32 meets our expectations of increase efficiency with increase problem size. Similarly, speed-up is more drastic with bigger problem but flattens out quickly with small problems.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{"../../../../../../Downloads/Speed-Up vs. Threads"}
    \caption{Speed-Up per number of processors per problem size. n is the dimensions of the square \(n \times n\) matrix}
    \label{fig:speed-up-vs}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{"../../../../../../Downloads/Efficiency vs. Threads"}
    \caption{Efficiency per number of processors per problem size. n is the dimensions of the square \(n \times n\) matrix}
    \label{fig:efficiency-vs}
\end{figure}


\section*{Gauss-Elimination}
The second method we tested was Gauss-Elimination. This methods uses a series of row-operations to generate L and U.  The algorithm first finds a pivot for column \(i\), scales it by the pivot, then subtracts the \(i+1\) row. For Gaussian's MPI implementation, we ran into issues with sending and receiving the data within Eigen data, which is the library we used to represent our matrices. Firstly, Eigen matrices are more complex data structures than MPI natively supports. For any communication to happen we have to ensure that we are extracting native C arrays and sending those instead. Eigen does have a Matrix.row(number).data() function that can be used for this purpose. The next issue was that Eigen by default stores data internally in column major order, which means data is stored such that columns are traversed before rows. I experimented with storing the data in Row Major order, but it only caused issues on the data ingestion and didn't seem to improve accuracy or performance, but there is some impact there that can be explored. Finally, we ran into an issue with the allocation of memory between processes. Our data loader (matrix\_reader.cpp) dynamically reads in a CSV file and allocates the memory space using a dynamically sized matrix object on the fly. This approach, by default, is not going to work for broadcasting data between processes for MPI. We solved this by requiring size as an input for the Gaussian MPI program, and pre-allocating the space in each process before ingesting the data into the allocated space.
\end{document}
